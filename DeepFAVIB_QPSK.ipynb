{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNC89nnG/0857++YB2j93s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srijani19/Testform1/blob/main/DeepFAVIB_QPSK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SgFhX9YI3BMo",
        "outputId": "0aacb4d6-bc7e-450a-d4c6-4fea501481c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2449253114.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use TF1-style graph execution (disable eager mode in TF2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/__internal__/feature_column/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseColumn\u001b[0m \u001b[0;31m# line: 1777\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureTransformationCache\u001b[0m \u001b[0;31m# line: 1962\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceDenseColumn\u001b[0m \u001b[0;31m# line: 1941\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_v2_types\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mInputSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0mHDF5_OBJECT_HEADER_LIMIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/h5py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# We tried working around this by using \"package_dir\" but that breaks Cython.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Uniform QPSK, N=8 (No WGAN)\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use TF1-style graph execution (disable eager mode in TF2)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ——— Hyperparameters ———\n",
        "TRAIN_SNRs = [0, 4, 10, 13]         # SNRs (in dB) used for training\n",
        "TEST_SNRs  = np.linspace(0, 15, 16) # SNR sweep (0→15 dB) for evaluation\n",
        "N_train    = N_test = 100_000       # Number of training & test samples\n",
        "N_z        = 8                      # Latent categorical variable size\n",
        "hidden_dims= [300, 200, 100]        # MLP hidden layer sizes (encoder/decoder)\n",
        "λ          = 0.05                   # KL divergence regularization weight\n",
        "τ          = 1.0                    # Gumbel-softmax temperature\n",
        "ε          = 0.01                   # Forward-link error probability\n",
        "batch_size = 2000                   # Mini-batch size\n",
        "epochs     = 1000                   # Training epochs per SNR\n",
        "lr         = 1e-3                   # Learning rate\n",
        "\n",
        "# Define QPSK constellation points (Gray-mapped, normalized to unit power)\n",
        "const = np.array([[1,1],[-1,1],[-1,-1],[1,-1]], np.float32)/np.sqrt(2)\n",
        "\n",
        "# ---------------- Dense Layer Helper ----------------\n",
        "def dense(x, in_dim, out_dim, name, act=tf.nn.relu):\n",
        "    W = tf.Variable(tf.random.normal([in_dim,out_dim], stddev=1/np.sqrt(in_dim/2)), name='W_'+name)\n",
        "    b = tf.Variable(tf.zeros([out_dim]), name='b_'+name)\n",
        "    y = tf.matmul(x,W)+b\n",
        "    return act(y) if act else y\n",
        "\n",
        "# ---------------- Build Encoder/Decoder Graph ----------------\n",
        "def build_model():\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    x_ph = tf.compat.v1.placeholder(tf.float32, [None,2], name='x')\n",
        "    y_ph = tf.compat.v1.placeholder(tf.int32,   [None],   name='y')\n",
        "    is_tr = tf.compat.v1.placeholder_with_default(True, shape=(), name='is_tr')\n",
        "\n",
        "    # Encoder\n",
        "    h = x_ph\n",
        "    for i,d in enumerate(hidden_dims):\n",
        "        h = dense(h, int(h.shape[1]), d, f'enc{i}')\n",
        "    logits_z = dense(h, hidden_dims[-1], N_z, 'logitz', act=None)\n",
        "\n",
        "    # Gumbel-softmax\n",
        "    g = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits_z))+1e-20)+1e-20)\n",
        "    z_soft = tf.nn.softmax((logits_z+g)/τ)\n",
        "    z_hard = tf.one_hot(tf.argmax(logits_z,1), N_z)\n",
        "    z = tf.cond(is_tr, lambda:z_soft, lambda:z_hard)\n",
        "\n",
        "    # Forward-link errors\n",
        "    mask = tf.random.uniform(tf.shape(z)) < ε\n",
        "    rand_z = tf.one_hot(tf.random.uniform([tf.shape(z)[0]], 0, N_z, tf.int32), N_z)\n",
        "    z_noisy = tf.where(mask, rand_z, z)\n",
        "\n",
        "    # Decoder\n",
        "    h2 = z_noisy\n",
        "    for i,d in enumerate(hidden_dims):\n",
        "        h2 = dense(h2, int(h2.shape[1]), d, f'dec{i}')\n",
        "    logits_x = dense(h2, hidden_dims[-1], 4, 'logits_x', act=None)\n",
        "\n",
        "    # Loss\n",
        "    ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_ph,logits=logits_x))\n",
        "    qz = tf.nn.softmax(logits_z)\n",
        "    kl = tf.reduce_mean(tf.reduce_sum(qz*(tf.math.log(qz+1e-20)-tf.math.log(1.0/N_z)),axis=1))\n",
        "    loss = ce + λ*kl\n",
        "    train_op = tf.compat.v1.train.AdamOptimizer(lr).minimize(loss)\n",
        "\n",
        "    return x_ph, y_ph, is_tr, logits_x, loss, train_op\n",
        "\n",
        "# ---------------- Main Training & Evaluation ----------------\n",
        "def main():\n",
        "    x_ph, y_ph, is_tr, logits_x, loss, train_op = build_model()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    results = {}\n",
        "\n",
        "    for snr in TRAIN_SNRs:\n",
        "        print(f\"\\n=== TRAIN @ {snr} dB ===\")\n",
        "        sigma = np.sqrt(1/(2*10**(snr/10)))\n",
        "        tx = np.random.randint(0,4,N_train)\n",
        "        x_train = const[tx] + sigma*np.random.randn(N_train,2)\n",
        "\n",
        "        # Training loop\n",
        "        for ep in range(epochs):\n",
        "            idx = np.random.choice(N_train, batch_size, replace=False)\n",
        "            sess.run(train_op, {x_ph:x_train[idx], y_ph:tx[idx], is_tr:True})\n",
        "            if ep%200==0:\n",
        "                lval = sess.run(loss, {x_ph:x_train[idx], y_ph:tx[idx], is_tr:True})\n",
        "                print(f\"  epoch {ep:4d}, loss {lval:.4f}\")\n",
        "\n",
        "        # Evaluation\n",
        "        ser = []\n",
        "        for s in TEST_SNRs:\n",
        "            sigte = np.sqrt(1/(2*10**(s/10)))\n",
        "            tx_te = np.random.randint(0,4,N_test)\n",
        "            x_te = const[tx_te] + sigte*np.random.randn(N_test,2)\n",
        "            preds = []\n",
        "            for i in range(0,N_test,batch_size):\n",
        "                p = sess.run(tf.argmax(logits_x,1), {x_ph:x_te[i:i+batch_size], is_tr:False})\n",
        "                preds.append(p)\n",
        "            preds = np.concatenate(preds)\n",
        "            ser.append(np.mean(preds!=tx_te))\n",
        "        results[snr] = ser\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for snr, ser in results.items():\n",
        "        plt.semilogy(TEST_SNRs, ser, 'o-', label=f\"train @ {snr} dB\")\n",
        "    plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "    plt.xlabel(\"Test SNR (dB)\")\n",
        "    plt.ylabel(\"SER\")\n",
        "    plt.title(\"Fig 4(b) – Deep FAVIB (ε=0.01)\")\n",
        "    plt.xlim(0,17.5)\n",
        "    plt.ylim(1e-5,1)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uniform QPSK, N=8 (With WGAN-GP)\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use TF1 graph mode (for TF2 environments)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ======================== Parameters =========================\n",
        "TRAIN_SNRs = [0, 4, 10, 13]   # SNRs (in dB) used for training\n",
        "TEST_SNRs  = np.linspace(0, 15, 16)  # SNR sweep for evaluation\n",
        "\n",
        "N_train, N_test = int(1e5), int(1e5)  # dataset sizes\n",
        "\n",
        "N_z = 8                 # latent categorical size\n",
        "hidden_dims = [100, 50] # encoder/decoder MLP widths\n",
        "\n",
        "# Loss weights and training knobs\n",
        "lambda_kl   = 0.01\n",
        "beta_gp     = 1.0\n",
        "g_loss_scale= 0.01\n",
        "batch_size  = 2000\n",
        "epochs      = 1000\n",
        "tau         = 1.0\n",
        "epsilon     = 0.01\n",
        "lr          = 1e-4\n",
        "\n",
        "# QPSK constellation\n",
        "const = np.array([[ 1, 1],\n",
        "                  [-1, 1],\n",
        "                  [-1,-1],\n",
        "                  [ 1,-1]], dtype=np.float32) / np.sqrt(2)\n",
        "\n",
        "# =================== Dense Layer Helper ===================\n",
        "def dense(input, in_dim, out_dim, name, activation=tf.nn.relu):\n",
        "    W = tf.Variable(\n",
        "        tf.random.normal([in_dim, out_dim], stddev=1/np.sqrt(in_dim/2)),\n",
        "        name=f\"W_{name}\"\n",
        "    )\n",
        "    b = tf.Variable(tf.zeros([out_dim]), name=f\"b_{name}\")\n",
        "    out = tf.matmul(input, W) + b\n",
        "    return activation(out) if activation else out\n",
        "\n",
        "# ========================= Model Graph ==========================\n",
        "def build_model():\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    x = tf.compat.v1.placeholder(tf.float32, [None, 2])   # noisy I/Q input\n",
        "    y = tf.compat.v1.placeholder(tf.int32,   [None])      # true QPSK index\n",
        "    is_training = tf.compat.v1.placeholder_with_default(True, shape=())\n",
        "\n",
        "    # ----- Encoder -----\n",
        "    h = x\n",
        "    for i, dim in enumerate(hidden_dims):\n",
        "        h = dense(h, int(h.shape[1]), dim, f\"enc{i}\")\n",
        "    logits_z = dense(h, int(h.shape[1]), N_z, \"logitz\", activation=None)\n",
        "\n",
        "    # ----- Gumbel-Softmax -----\n",
        "    g = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits_z)) + 1e-20))\n",
        "    z_soft = tf.nn.softmax((logits_z + g) / tau)\n",
        "    z_hard = tf.one_hot(tf.argmax(logits_z, axis=1), N_z)\n",
        "    z = tf.cond(is_training, lambda: z_soft, lambda: z_hard)\n",
        "\n",
        "    # ----- Forward channel latent noise -----\n",
        "    noise_mask = tf.random.uniform(tf.shape(z)) < epsilon\n",
        "    z_rand = tf.one_hot(\n",
        "        tf.random.uniform([tf.shape(z)[0]], 0, N_z, dtype=tf.int32),\n",
        "        N_z\n",
        "    )\n",
        "    z_noisy = tf.where(noise_mask, z_rand, z)\n",
        "\n",
        "    # ----- Decoder -----\n",
        "    h_dec = z_noisy\n",
        "    for i, dim in enumerate(hidden_dims):\n",
        "        h_dec = dense(h_dec, int(h_dec.shape[1]), dim, f\"dec{i}\")\n",
        "    logits_x = dense(h_dec, int(h_dec.shape[1]), 4, \"logits_x\", activation=None)\n",
        "\n",
        "    # =================== WGAN-GP Critic ===================\n",
        "    z_real = tf.one_hot(\n",
        "        tf.random.uniform([batch_size], 0, N_z, dtype=tf.int32),\n",
        "        N_z\n",
        "    )\n",
        "\n",
        "    def critic(z_in):\n",
        "        h = dense(z_in, N_z, 50, \"d1\")\n",
        "        return dense(h, 50, 1, \"d2\", activation=None)\n",
        "\n",
        "    D_real = critic(z_real)\n",
        "    D_fake = critic(z_soft)\n",
        "\n",
        "    # Gradient Penalty\n",
        "    alpha   = tf.random.uniform([batch_size, 1])\n",
        "    interp  = alpha * z_real + (1 - alpha) * z_soft\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interp)\n",
        "        D_interp = critic(interp)\n",
        "    grads     = tape.gradient(D_interp, interp)\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1) + 1e-8)\n",
        "    gp        = tf.reduce_mean(tf.square(grad_norm - 1.0))\n",
        "\n",
        "    # ======================== Losses ============================\n",
        "    ce = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_x)\n",
        "    )\n",
        "    q_z = tf.nn.softmax(logits_z)\n",
        "    kl  = tf.reduce_mean(\n",
        "        tf.reduce_sum(q_z * (tf.math.log(q_z + 1e-20) - tf.math.log(1.0 / N_z)), axis=1)\n",
        "    )\n",
        "    d_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + beta_gp * gp\n",
        "    g_loss = -tf.reduce_mean(D_fake)\n",
        "\n",
        "    loss = ce + lambda_kl * kl + g_loss_scale * g_loss\n",
        "\n",
        "    all_vars = tf.compat.v1.trainable_variables()\n",
        "    d_vars   = [v for v in all_vars if \"d\" in v.name]\n",
        "    g_vars   = [v for v in all_vars if (\"enc\" in v.name or \"dec\" in v.name or \"logitz\" in v.name)]\n",
        "\n",
        "    train_op_d = tf.compat.v1.train.AdamOptimizer(lr).minimize(d_loss, var_list=d_vars)\n",
        "    train_op_g = tf.compat.v1.train.AdamOptimizer(lr).minimize(loss,   var_list=g_vars)\n",
        "\n",
        "    return x, y, is_training, logits_x, loss, train_op_g, train_op_d\n",
        "\n",
        "# ===================== Training & Evaluation =====================\n",
        "def run():\n",
        "    x_ph, y_ph, is_tr, logits_x, loss, train_op_g, train_op_d = build_model()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    results = {}\n",
        "\n",
        "    for snr in TRAIN_SNRs:\n",
        "        print(f\"\\nTraining @ {snr} dB\")\n",
        "        σ = np.sqrt(1 / (2 * 10**(snr / 10)))\n",
        "        tx = np.random.randint(0, 4, N_train)\n",
        "        x_train = const[tx] + σ * np.random.randn(N_train, 2)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            idx = np.random.choice(N_train, batch_size)\n",
        "            xb, yb = x_train[idx], tx[idx]\n",
        "            if ep % 5 == 0:\n",
        "                sess.run(train_op_d, {x_ph: xb, y_ph: yb, is_tr: True})\n",
        "            sess.run(train_op_g, {x_ph: xb, y_ph: yb, is_tr: True})\n",
        "\n",
        "            if ep % 200 == 0:\n",
        "                lv = sess.run(loss, {x_ph: xb, y_ph: yb, is_tr: True})\n",
        "                print(f\"Epoch {ep}, Loss = {lv:.4f}\")\n",
        "\n",
        "        # ----- Evaluation -----\n",
        "        ser = []\n",
        "        for snr_te in TEST_SNRs:\n",
        "            σ_te = np.sqrt(1 / (2 * 10**(snr_te / 10)))\n",
        "            tx_te = np.random.randint(0, 4, N_test)\n",
        "            x_te  = const[tx_te] + σ_te * np.random.randn(N_test, 2)\n",
        "\n",
        "            preds = []\n",
        "            for i in range(0, N_test, batch_size):\n",
        "                xb = x_te[i:i + batch_size]\n",
        "                preds.extend(sess.run(tf.argmax(logits_x, axis=1),\n",
        "                                      {x_ph: xb, is_tr: False}))\n",
        "            ser.append(np.mean(np.array(preds) != tx_te))\n",
        "\n",
        "        results[snr] = ser\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================ Run ===============================\n",
        "results = run()\n",
        "\n",
        "# ============================ Plot ==============================\n",
        "plt.figure(figsize=(8, 6))\n",
        "for snr, ser in results.items():\n",
        "    plt.semilogy(TEST_SNRs, ser, marker='o', label=f\"Train @ {snr} dB\")\n",
        "plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "plt.xlabel(\"Test SNR (dB)\")\n",
        "plt.ylabel(\"SER\")\n",
        "plt.title(\"Figure 4(b) – Deep FAVIB + Balanced WGAN-GP\")\n",
        "plt.legend()\n",
        "plt.ylim(1e-4, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o8p2wtFw4gEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Uniform QPSK, N=8 (No WGAN)\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---------------- TF Setup ----------------\n",
        "tf.compat.v1.disable_eager_execution()   # run in TF1-style graph mode\n",
        "np.random.seed(0)                        # numpy reproducibility\n",
        "tf.compat.v1.set_random_seed(0)          # tensorflow reproducibility\n",
        "\n",
        "# ---------------- Experiment Parameters ----------------\n",
        "TRAIN_SNRs = [0, 4, 10, 13]              # training SNRs in dB\n",
        "TEST_SNRs  = np.linspace(0, 15, 16)      # sweep of SNRs for evaluation\n",
        "N_train = N_test = 100_000               # number of samples\n",
        "\n",
        "N_z = 8                                  # latent categorical size\n",
        "hidden_dims = [100, 50]                  # encoder/decoder hidden layers\n",
        "lambda_kl = 0.1                          # weight for KL divergence term\n",
        "tau_start, tau_end = 1.0, 0.7            # Gumbel-Softmax τ (annealed)\n",
        "epsilon = 0.01                           # forward-link latent noise\n",
        "\n",
        "batch_size = 2000                        # mini-batch size\n",
        "epochs = 1000                            # training epochs\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_g_start, lr_g_end = 3e-4, 1e-4\n",
        "\n",
        "# ---------------- Non-uniform priors ----------------\n",
        "latent_probs = np.array(\n",
        "    [0.30, 0.20, 0.15, 0.10, 0.08, 0.07, 0.06, 0.04], np.float32\n",
        ")\n",
        "latent_probs /= latent_probs.sum()\n",
        "\n",
        "# QPSK constellation\n",
        "const = np.array([\n",
        "    [ 1,  1],\n",
        "    [-1,  1],\n",
        "    [-1, -1],\n",
        "    [ 1, -1]\n",
        "], np.float32) / np.sqrt(2.0)\n",
        "\n",
        "M = 4\n",
        "p_qpsk = np.array([0.50, 0.20, 0.20, 0.10], np.float32)  # non-uniform source probs\n",
        "\n",
        "# ---------------- Dense Layer ----------------\n",
        "def dense(x, in_dim, out_dim, name, act=tf.nn.relu):\n",
        "    W = tf.Variable(\n",
        "        tf.random.normal([in_dim, out_dim], stddev=1/np.sqrt(max(in_dim/2, 1))),\n",
        "        name=f\"W_{name}\"\n",
        "    )\n",
        "    b = tf.Variable(tf.zeros([out_dim]), name=f\"b_{name}\")\n",
        "    y = tf.matmul(x, W) + b\n",
        "    return act(y) if act else y\n",
        "\n",
        "# ---------------- Build Model ----------------\n",
        "def build_model():\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    x = tf.compat.v1.placeholder(tf.float32, [None, 2], name=\"x\")\n",
        "    y = tf.compat.v1.placeholder(tf.int32,   [None],   name=\"y\")\n",
        "    is_tr  = tf.compat.v1.placeholder_with_default(True, shape=(), name=\"is_tr\")\n",
        "    tau_ph = tf.compat.v1.placeholder_with_default(tau_start, shape=(), name=\"tau\")\n",
        "    lr_ph  = tf.compat.v1.placeholder_with_default(lr_g_start, shape=(), name=\"lr\")\n",
        "\n",
        "    prior = tf.constant(latent_probs, tf.float32)\n",
        "\n",
        "    # Encoder\n",
        "    h = x\n",
        "    for i, d in enumerate(hidden_dims):\n",
        "        h = dense(h, int(h.shape[1]), d, f\"enc{i}\")\n",
        "    logits_z = dense(h, int(h.shape[1]), N_z, \"logitz\", act=None)\n",
        "\n",
        "    # Gumbel–Softmax\n",
        "    g = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits_z)) + 1e-20) + 1e-20)\n",
        "    z_soft = tf.nn.softmax((logits_z + g) / tau_ph)\n",
        "    z_hard = tf.one_hot(tf.argmax(logits_z, axis=1), N_z)\n",
        "    z_clean = tf.cond(is_tr, lambda: z_soft, lambda: z_hard)\n",
        "\n",
        "    # Forward channel latent flips\n",
        "    noise_mask = tf.random.uniform(tf.shape(z_clean)) < epsilon\n",
        "    lat_logp   = tf.math.log(prior[None, :] + 1e-20)\n",
        "    z_rand_idx = tf.random.categorical(lat_logp, tf.shape(z_clean)[0])[:, 0]\n",
        "    z_rand     = tf.one_hot(z_rand_idx, N_z)\n",
        "    z_used     = tf.cond(is_tr, lambda: z_clean, lambda: tf.where(noise_mask, z_rand, z_clean))\n",
        "\n",
        "    # Decoder\n",
        "    h2 = z_used\n",
        "    for i, d in enumerate(hidden_dims):\n",
        "        h2 = dense(h2, int(h2.shape[1]), d, f\"dec{i}\")\n",
        "    logits_x = dense(h2, int(h2.shape[1]), M, \"logits_x\", act=None)\n",
        "\n",
        "    # Loss: CE + KL(q||r)\n",
        "    ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_x))\n",
        "    qz = tf.nn.softmax(logits_z)\n",
        "    kl_q_r = tf.reduce_mean(\n",
        "        tf.reduce_sum(qz * (tf.math.log(qz + 1e-20) - tf.math.log(prior + 1e-20)), axis=1)\n",
        "    )\n",
        "    loss = ce + lambda_kl * kl_q_r\n",
        "    train_op = tf.compat.v1.train.AdamOptimizer(lr_ph, beta1=0.5, beta2=0.9).minimize(loss)\n",
        "\n",
        "    entropy_q = -tf.reduce_mean(tf.reduce_sum(qz * tf.math.log(qz + 1e-20), axis=1))\n",
        "    perplexity = tf.exp(entropy_q)\n",
        "\n",
        "    return x, y, is_tr, tau_ph, lr_ph, logits_x, ce, kl_q_r, loss, train_op, perplexity\n",
        "\n",
        "# ---------------- Training & Evaluation ----------------\n",
        "def run():\n",
        "    (x_ph, y_ph, is_tr, tau_ph, lr_ph,\n",
        "     logits_x, ce_t, kl_t, loss_t, train_op, perp_t) = build_model()\n",
        "\n",
        "    sess = tf.compat.v1.Session()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    results = {}\n",
        "\n",
        "    for snr in TRAIN_SNRs:\n",
        "        print(f\"\\n=== TRAIN @ {snr} dB (NO-GAN; params match WGAN) ===\")\n",
        "        sigma = np.sqrt(1.0 / (2.0 * 10.0**(snr / 10.0)))\n",
        "        tx = np.random.choice(M, size=N_train, p=p_qpsk)\n",
        "        x_train = const[tx] + sigma * np.random.randn(N_train, 2).astype(np.float32)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            frac = ep / max(epochs - 1, 1)\n",
        "            lr_now  = lr_g_end + 0.5 * (lr_g_start - lr_g_end) * (1 + np.cos(np.pi * frac))\n",
        "            tau_now = tau_end +      (tau_start - tau_end) * (1 - frac)\n",
        "\n",
        "            idx = np.random.choice(N_train, batch_size, replace=False)\n",
        "            xb, yb = x_train[idx], tx[idx]\n",
        "            sess.run(train_op, {x_ph: xb, y_ph: yb, is_tr: True, tau_ph: tau_now, lr_ph: lr_now})\n",
        "\n",
        "            if ep % 200 == 0:\n",
        "                ce_v, kl_v, loss_v, perp_v = sess.run(\n",
        "                    [ce_t, kl_t, loss_t, perp_t],\n",
        "                    {x_ph: xb, y_ph: yb, is_tr: True, tau_ph: tau_now}\n",
        "                )\n",
        "                print(f\"  ep {ep:4d} | CE {ce_v:.4f}  KL(q||r) {kl_v:.4f}  \"\n",
        "                      f\"Tot {loss_v:.4f}  Perp~{perp_v:.2f}\")\n",
        "\n",
        "        # SER evaluation\n",
        "        ser = []\n",
        "        for s in TEST_SNRs:\n",
        "            sigma_te = np.sqrt(1.0 / (2.0 * 10.0**(s / 10.0)))\n",
        "            tx_te = np.random.choice(M, size=N_test, p=p_qpsk)\n",
        "            x_te  = const[tx_te] + sigma_te * np.random.randn(N_test, 2).astype(np.float32)\n",
        "\n",
        "            preds = []\n",
        "            for i in range(0, N_test, batch_size):\n",
        "                xb = x_te[i:i+batch_size]\n",
        "                p  = sess.run(tf.argmax(logits_x, axis=1), {x_ph: xb, is_tr: False})\n",
        "                preds.append(p)\n",
        "            preds = np.concatenate(preds)\n",
        "            ser.append(np.mean(preds != tx_te))\n",
        "        results[snr] = ser\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------------- Run & Plot ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    results = run()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for snr, ser in results.items():\n",
        "        plt.semilogy(TEST_SNRs, ser, marker='o', label=f\"Train @ {snr} dB\")\n",
        "    plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "    plt.xlabel(\"Test SNR (dB)\")\n",
        "    plt.ylabel(\"SER\")\n",
        "    plt.title(\"Deep FAVIB (NO GAN) — Non-Uniform QPSK\")\n",
        "    plt.legend()\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3jWv1D0c4z_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Uniform QPSK, N=8 (With WGAN-GP)\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----- TF1 graph mode (compatible inside TF2) -----\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ----- Reproducibility -----\n",
        "np.random.seed(0)\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "\n",
        "# ---------------- Experiment Parameters ----------------\n",
        "TRAIN_SNRs = [0, 4, 10, 13]          # training SNRs (dB)\n",
        "TEST_SNRs = np.linspace(0, 15, 16)   # evaluation SNR sweep (dB)\n",
        "N_train = N_test = 100_000           # samples per training/test run\n",
        "\n",
        "N_z = 8                              # latent categorical dimension\n",
        "hidden_dims = [100, 50]              # encoder/decoder hidden layers\n",
        "lambda_kl = 0.1                      # weight for KL(q||r)\n",
        "tau_start, tau_end = 1.0, 0.7        # Gumbel-Softmax τ schedule (annealed)\n",
        "epsilon = 0.01                       # latent flip probability (test-time forward noise)\n",
        "\n",
        "batch_size = 2000                    # batch size\n",
        "epochs = 1000                        # epochs per SNR run\n",
        "\n",
        "# Learning rate schedules (cosine decay, match NO-GAN baseline)\n",
        "lr_g_start, lr_g_end = 3e-4, 1e-4    # generator LR schedule\n",
        "lr_d_start, lr_d_end = 3e-4, 1e-4    # critic LR schedule\n",
        "\n",
        "# WGAN-GP specific params\n",
        "critic_steps = 3                     # critic updates per generator update\n",
        "beta_gp = 10.0                       # gradient penalty weight\n",
        "g_loss_scale = 0.01                  # weight of adversarial term in generator loss\n",
        "\n",
        "# ---------------- Latent prior r(z) ----------------\n",
        "latent_probs = np.array([0.30, 0.20, 0.15, 0.10,\n",
        "                         0.08, 0.07, 0.06, 0.04], np.float32)\n",
        "latent_probs /= latent_probs.sum()\n",
        "\n",
        "# ---------------- Modulation: Non-uniform QPSK ----------------\n",
        "const = np.array([[1, 1],\n",
        "                  [-1, 1],\n",
        "                  [-1, -1],\n",
        "                  [1, -1]], np.float32) / np.sqrt(2.0)\n",
        "M = 4\n",
        "p_qpsk = np.array([0.50, 0.20, 0.20, 0.10], np.float32)\n",
        "\n",
        "# ---------------- Dense Layer ----------------\n",
        "def dense(x, in_dim, out_dim, name, act=tf.nn.relu):\n",
        "    W = tf.Variable(\n",
        "        tf.random.normal([in_dim, out_dim], stddev=1 / np.sqrt(max(in_dim / 2, 1))),\n",
        "        name=f\"W_{name}\"\n",
        "    )\n",
        "    b = tf.Variable(tf.zeros([out_dim]), name=f\"b_{name}\")\n",
        "    y = tf.matmul(x, W) + b\n",
        "    return act(y) if act is not None else y\n",
        "\n",
        "# ---------------- Build Model ----------------\n",
        "def build_model():\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    # --- Placeholders ---\n",
        "    x = tf.compat.v1.placeholder(tf.float32, [None, 2], name=\"x\")     # noisy I/Q input\n",
        "    y = tf.compat.v1.placeholder(tf.int32, [None], name=\"y\")          # ground-truth symbols\n",
        "    is_tr = tf.compat.v1.placeholder_with_default(True, shape=(), name=\"is_tr\")    # training flag\n",
        "    tau_ph = tf.compat.v1.placeholder_with_default(tau_start, shape=(), name=\"tau\")  # Gumbel τ\n",
        "    lr_g = tf.compat.v1.placeholder_with_default(lr_g_start, shape=(), name=\"lr_g\")  # gen LR\n",
        "    lr_d = tf.compat.v1.placeholder_with_default(lr_d_start, shape=(), name=\"lr_d\")  # critic LR\n",
        "\n",
        "    prior = tf.constant(latent_probs, tf.float32)\n",
        "\n",
        "    # --- Encoder: x -> logits over latent categories ---\n",
        "    h = x\n",
        "    for i, d in enumerate(hidden_dims):\n",
        "        h = dense(h, int(h.shape[1]), d, f\"enc{i}\")\n",
        "    logits_z = dense(h, int(h.shape[1]), N_z, \"logitz\", act=None)\n",
        "\n",
        "    # --- Gumbel-Softmax bottleneck ---\n",
        "    g = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits_z)) + 1e-20) + 1e-20)\n",
        "    z_soft = tf.nn.softmax((logits_z + g) / tau_ph)        # training: soft sample\n",
        "    z_hard = tf.one_hot(tf.argmax(logits_z, axis=1), N_z)  # eval: hard one-hot\n",
        "    z_clean = tf.cond(is_tr, lambda: z_soft, lambda: z_hard)\n",
        "\n",
        "    # --- Forward channel latent noise at test-time ---\n",
        "    noise_mask = tf.random.uniform(tf.shape(z_clean)) < epsilon\n",
        "    lat_logp = tf.math.log(prior[None, :] + 1e-20)\n",
        "    z_rand_idx = tf.random.categorical(lat_logp, tf.shape(z_clean)[0])[:, 0]\n",
        "    z_rand = tf.one_hot(z_rand_idx, N_z)\n",
        "    z_used = tf.cond(is_tr, lambda: z_clean,\n",
        "                     lambda: tf.where(noise_mask, z_rand, z_clean))\n",
        "\n",
        "    # --- Decoder: latent -> logits over QPSK symbols ---\n",
        "    h2 = z_used\n",
        "    for i, d in enumerate(hidden_dims):\n",
        "        h2 = dense(h2, int(h2.shape[1]), d, f\"dec{i}\")\n",
        "    logits_x = dense(h2, int(h2.shape[1]), M, \"logits_x\", act=None)\n",
        "\n",
        "    # --- Task/regularization losses ---\n",
        "    ce = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_x)\n",
        "    )\n",
        "    qz = tf.nn.softmax(logits_z)\n",
        "    kl_q_r = tf.reduce_mean(\n",
        "        tf.reduce_sum(qz * (tf.math.log(qz + 1e-20) - tf.math.log(prior + 1e-20)), axis=1)\n",
        "    )\n",
        "\n",
        "    # ---------------- WGAN-GP critic in latent space ----------------\n",
        "    with tf.compat.v1.variable_scope(\"critic\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "        def critic(z_in):\n",
        "            h = dense(z_in, N_z, 64, \"d1\")\n",
        "            return dense(h, 64, 1, \"d2\", act=None)\n",
        "\n",
        "        # Real latent codes sampled from prior r(z)\n",
        "        z_real_idx = tf.random.categorical(tf.math.log(prior[None, :]), batch_size)[:, 0]\n",
        "        z_real = tf.one_hot(z_real_idx, N_z)\n",
        "\n",
        "        # Critic scores\n",
        "        D_real = critic(z_real)                             # score for real codes\n",
        "        D_fake_detached = critic(tf.stop_gradient(z_soft))  # score for fake codes (detached)\n",
        "\n",
        "        # Gradient penalty on interpolations\n",
        "        alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
        "        interp = alpha * z_real + (1.0 - alpha) * tf.stop_gradient(z_soft)\n",
        "        D_interp = critic(interp)\n",
        "        grads = tf.gradients(D_interp, [interp])[0]\n",
        "        grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1) + 1e-8)\n",
        "        gp = tf.reduce_mean(tf.square(grad_norm - 1.0))\n",
        "\n",
        "        # Critic loss: E[D(fake)] - E[D(real)] + λ_gp * GP\n",
        "        d_loss = tf.reduce_mean(D_fake_detached) - tf.reduce_mean(D_real) + beta_gp * gp\n",
        "\n",
        "        # Generator adversarial term uses non-detached z_soft\n",
        "        D_fake_for_G = critic(z_soft)\n",
        "        g_wgan = -tf.reduce_mean(D_fake_for_G)\n",
        "\n",
        "    # --- Total generator loss (task + reg + adversarial) ---\n",
        "    total_gen_loss = ce + lambda_kl * kl_q_r + g_loss_scale * g_wgan\n",
        "\n",
        "    # --- Variable partitions ---\n",
        "    d_vars = tf.compat.v1.get_collection(\n",
        "        tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='critic'\n",
        "    )\n",
        "    g_vars = [v for v in tf.compat.v1.trainable_variables() if v not in d_vars]\n",
        "\n",
        "    # --- Optimizers ---\n",
        "    train_op_d = tf.compat.v1.train.AdamOptimizer(lr_d, beta1=0.5, beta2=0.9).minimize(\n",
        "        d_loss, var_list=d_vars\n",
        "    )\n",
        "    train_op_g = tf.compat.v1.train.AdamOptimizer(lr_g, beta1=0.5, beta2=0.9).minimize(\n",
        "        total_gen_loss, var_list=g_vars\n",
        "    )\n",
        "\n",
        "    # Diagnostics\n",
        "    entropy_q = -tf.reduce_mean(tf.reduce_sum(qz * tf.math.log(qz + 1e-20), axis=1))\n",
        "    perplexity = tf.exp(entropy_q)\n",
        "\n",
        "    return (x, y, is_tr, tau_ph, lr_g, lr_d,\n",
        "            logits_x, ce, kl_q_r, d_loss, g_wgan, total_gen_loss,\n",
        "            train_op_g, train_op_d, perplexity)\n",
        "\n",
        "# ---------------- Training & Evaluation ----------------\n",
        "def run():\n",
        "    (x_ph, y_ph, is_tr, tau_ph, lr_g_ph, lr_d_ph,\n",
        "     logits_x, ce_t, kl_t, dloss_t, gw_t, tot_t,\n",
        "     train_op_g, train_op_d, perp_t) = build_model()\n",
        "\n",
        "    sess = tf.compat.v1.Session()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Train at each specified SNR\n",
        "    for snr in TRAIN_SNRs:\n",
        "        print(f\"\\n=== TRAIN @ {snr} dB (WGAN-GP; params match baseline) ===\")\n",
        "\n",
        "        sigma = np.sqrt(1.0 / (2.0 * 10.0**(snr / 10.0)))\n",
        "        tx = np.random.choice(M, size=N_train, p=p_qpsk)\n",
        "        x_train = const[tx] + sigma * np.random.randn(N_train, 2).astype(np.float32)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            # Cosine LR decay; τ anneals linearly\n",
        "            frac = ep / max(epochs - 1, 1)\n",
        "            lr_g_now = lr_g_end + 0.5 * (lr_g_start - lr_g_end) * (1 + np.cos(np.pi * frac))\n",
        "            lr_d_now = lr_d_end + 0.5 * (lr_d_start - lr_d_end) * (1 + np.cos(np.pi * frac))\n",
        "            tau_now = tau_end + (tau_start - tau_end) * (1 - frac)\n",
        "\n",
        "            idx = np.random.choice(N_train, batch_size, replace=False)\n",
        "            xb, yb = x_train[idx], tx[idx]\n",
        "\n",
        "            # Critic updates\n",
        "            for _ in range(critic_steps):\n",
        "                sess.run(\n",
        "                    train_op_d,\n",
        "                    {x_ph: xb, y_ph: yb, is_tr: True, tau_ph: tau_now,\n",
        "                     lr_g_ph: lr_g_now, lr_d_ph: lr_d_now}\n",
        "                )\n",
        "\n",
        "            # Generator update\n",
        "            sess.run(\n",
        "                train_op_g,\n",
        "                {x_ph: xb, y_ph: yb, is_tr: True, tau_ph: tau_now,\n",
        "                 lr_g_ph: lr_g_now, lr_d_ph: lr_d_now}\n",
        "            )\n",
        "\n",
        "            if ep % 200 == 0:\n",
        "                ce_v, kl_v, d_v, gw_v, tot_v, perp_v = sess.run(\n",
        "                    [ce_t, kl_t, dloss_t, gw_t, tot_t, perp_t],\n",
        "                    {x_ph: xb, y_ph: yb, is_tr: True, tau_ph: tau_now}\n",
        "                )\n",
        "                print(f\"  ep {ep:4d} | CE {ce_v:.4f}  KL {kl_v:.4f}  \"\n",
        "                      f\"D {d_v:.4f}  G_wgan {gw_v:.4f}  Tot {tot_v:.4f}  Perp~{perp_v:.2f}\")\n",
        "\n",
        "        # --- SER evaluation over the test SNR sweep ---\n",
        "        ser = []\n",
        "        for s in TEST_SNRs:\n",
        "            sigma_te = np.sqrt(1.0 / (2.0 * 10.0**(s / 10.0)))\n",
        "            tx_te = np.random.choice(M, size=N_test, p=p_qpsk)\n",
        "            x_te = const[tx_te] + sigma_te * np.random.randn(N_test, 2).astype(np.float32)\n",
        "\n",
        "            preds = []\n",
        "            for i in range(0, N_test, batch_size):\n",
        "                xb = x_te[i:i + batch_size]\n",
        "                p = sess.run(tf.argmax(logits_x, axis=1), {x_ph: xb, is_tr: False})\n",
        "                preds.append(p)\n",
        "            preds = np.concatenate(preds)\n",
        "            ser.append(np.mean(preds != tx_te))\n",
        "\n",
        "        results[snr] = ser\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------------- Run & Plot ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    results = run()\n",
        "\n",
        "    # Plot SER vs test SNR for each training SNR\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for snr, ser in results.items():\n",
        "        plt.semilogy(TEST_SNRs, ser, marker='o', label=f\"Train @ {snr} dB\")\n",
        "    plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "    plt.xlabel(\"Test SNR (dB)\")\n",
        "    plt.ylabel(\"SER\")\n",
        "    plt.title(\"Deep FAVIB + WGAN-GP — Non-Uniform QPSK\")\n",
        "    plt.legend()\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IEzdSU9X5Gr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}